{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:37:07.164195Z",
     "start_time": "2018-04-27T09:37:02.834255Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import string\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from functools import lru_cache, partial\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import gensim\n",
    "import fastText\n",
    "import pymorphy2\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from keras import backend as K \n",
    "from keras.models import load_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Concatenate, Conv1D, MaxPooling1D, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPool1D, Permute, Add\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained stuff\n",
    "\n",
    "There are two difficult-to-replicate parts of the solution - 1) fasttext embeddings trained on opensuptitles 2) neural net. Set these flags to true and put OS data in the `data/ext` folder to redo these calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:37:16.795917Z",
     "start_time": "2018-04-27T09:37:16.792487Z"
    }
   },
   "outputs": [],
   "source": [
    "REDO_VECTORS = False\n",
    "REDO_NN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenSubtitles Vectors\n",
    "\n",
    "Here I trained fasttext embeddings on OpenSubtitles 2018 dataset. To replicate results you'll need to redo the training. Dataset was processed a little with `process_OS_data` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:37:17.304206Z",
     "start_time": "2018-04-27T09:37:17.299891Z"
    }
   },
   "outputs": [],
   "source": [
    " def process_OS_data(source, total):\n",
    "        table = str.maketrans({key: None for key in string.punctuation})\n",
    "        processed = source + '.processed'\n",
    "\n",
    "        with open(source, encoding='utf-8') as source_f, open(processed, 'w', encoding='utf-8') as processed_f:\n",
    "            for line in tqdm_notebook(source_f, total=total):\n",
    "                # Lowercase and remove puctuation\n",
    "                processed_f.write(line.lower().translate(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:37:28.489769Z",
     "start_time": "2018-04-27T09:37:17.464298Z"
    }
   },
   "outputs": [],
   "source": [
    "if REDO_VECTORS:\n",
    "    process_OS_data('data/ext/OpenSubtitles2018.en-ru.ru', 25910105)\n",
    "    fasttext = fastText.train_unsupervised(\n",
    "        'data/ext/OpenSubtitles2018.en-ru.ru',\n",
    "        model='skipgram',\n",
    "        thread=4,\n",
    "        dim=200\n",
    "    )\n",
    "    fasttext.save_model('data/ext/os_model_2018.bin')\n",
    "else:\n",
    "    fasttext = fastText.load_model('data/ext/os_model_2018.bin')\n",
    "    \n",
    "fasttext_dim = fasttext.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:37:29.385064Z",
     "start_time": "2018-04-27T09:37:28.494752Z"
    }
   },
   "outputs": [],
   "source": [
    "TEST_COLUMNS = ['context_id', 'context_2', 'context_1', 'context_0', 'reply_id', 'reply']\n",
    "TRAIN_COLUMNS = TEST_COLUMNS + ['label', 'confidence']\n",
    "\n",
    "LABELS_MAPPING = {\n",
    "    'good': 2,\n",
    "    'neutral': 1,\n",
    "    'bad': 0, \n",
    "}\n",
    "\n",
    "train = pd.read_csv('data/train.tsv', sep='\\t', header=None, quoting=csv.QUOTE_NONE, names=TRAIN_COLUMNS)\n",
    "final = pd.read_csv('data/final.tsv', sep='\\t', header=None, quoting=csv.QUOTE_NONE, names=TEST_COLUMNS)\n",
    "\n",
    "train['label'] = train.label.map(LABELS_MAPPING)\n",
    "\n",
    "train.fillna('', inplace=True)\n",
    "final.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:37:29.392767Z",
     "start_time": "2018-04-27T09:37:29.387240Z"
    }
   },
   "outputs": [],
   "source": [
    "def clear_text(text):        \n",
    "    text = re.sub(r'[^\\w-]+', ' ', text.lower().replace('ั', 'ะต'))\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def clear_data_inplace(df):\n",
    "    df['context_2'] = df.context_2.map(clear_text)\n",
    "    df['context_1'] = df.context_1.map(clear_text)\n",
    "    df['context_0'] = df.context_0.map(clear_text)\n",
    "    df['reply'] = df.reply.map(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:37:36.233693Z",
     "start_time": "2018-04-27T09:37:29.396362Z"
    }
   },
   "outputs": [],
   "source": [
    "clear_data_inplace(train)\n",
    "clear_data_inplace(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors of important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:37:40.559804Z",
     "start_time": "2018-04-27T09:37:36.236218Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer().fit(pd.concat([\n",
    "    train.context_2 + ' ' + train.context_1 + ' ' + train.context_0 + ' ' + train.reply,\n",
    "    final.context_2 + ' ' + final.context_1 + ' ' + final.context_0 + ' ' + final.reply,\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:37:40.574895Z",
     "start_time": "2018-04-27T09:37:40.562770Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_top_important_words(sent, vocab, idf_dict, n):\n",
    "    words = [(idf_dict[word], word) for word in sent.split() if word in vocab]\n",
    "    if len(words) <= n:\n",
    "        return [word for score, word in words]        \n",
    "    else:\n",
    "        nth = sorted(words, key=lambda x: -x[0])[:n][-1]\n",
    "        return [word for score, word in words if score >= nth[0]][:n]\n",
    "    \n",
    "    \n",
    "def get_vectors(words, fasttext, n):\n",
    "    a = np.zeros((n, fasttext_dim))\n",
    "    for i in range(len(words)):\n",
    "        a[i] = fasttext.get_word_vector(words[i])\n",
    "    return a\n",
    "\n",
    "\n",
    "def get_important_words(df, vectorizer, fasttext, n):\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    idf_dict = dict(zip(sorted(vectorizer.vocabulary_), vectorizer.idf_))\n",
    "    \n",
    "    words_getter = partial(get_top_important_words, vocab=vocab, idf_dict=idf_dict, n=n)\n",
    "    vectors_getter = partial(get_vectors, fasttext=fasttext, n=n)\n",
    "            \n",
    "    return np.concatenate([\n",
    "        np.array(df.context_2.map(words_getter).map(vectors_getter).tolist()),\n",
    "        np.array(df.context_1.map(words_getter).map(vectors_getter).tolist()),\n",
    "        np.array(df.context_0.map(words_getter).map(vectors_getter).tolist()),\n",
    "        np.array(df.reply.map(words_getter).map(vectors_getter).tolist()),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:38:42.903288Z",
     "start_time": "2018-04-27T09:37:40.577545Z"
    }
   },
   "outputs": [],
   "source": [
    "train_top_vectors = get_important_words(train, vectorizer, fasttext, n=5)\n",
    "final_top_vectors = get_important_words(final, vectorizer, fasttext, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammems features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:38:43.046433Z",
     "start_time": "2018-04-27T09:38:42.908195Z"
    }
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "grammes_keys = [\n",
    "    'NOUN', 'gen2', 'Fimp', 'Surn', 'Fixd', 'GNdr', 'impr', 'datv', 'ANim', 'Prdx', \n",
    "    'Sgtm', 'Adjx', 'ADJS', 'Pltm', 'accs', 'NUMB', 'Slng', 'past', 'Dmns', 'ADJF', \n",
    "    'Supr', 'Impx', 'voct', 'gent', 'Anum', 'LATN', 'Inmx', 'Impe', 'Dist', 'Abbr', \n",
    "    'actv', 'loc2', 'V-en', 'NPRO', 'plur', '3per', 'nomn', 'V-ey', 'sing', 'Litr', \n",
    "    'Ques', 'Prnt', 'incl', 'masc', 'Ms-f', 'Geox', 'PRTS', 'inan', 'Cmp2', 'INFN', \n",
    "    'ablt', 'tran', 'perf', 'indc', 'Coun', 'GRND', 'V-oy', 'PNCT', 'impf', 'PRCL', \n",
    "    'intr', 'Name', 'pres', 'Orgn', 'loct', 'Poss', 'Af-p', 'Anph', 'anim', 'Subx', \n",
    "    'ADVB', 'Apro', 'V-sh', 'excl', 'V-be', 'neut', 'Erro', 'Infr', 'Vpre', 'femn', \n",
    "    'futr', 'Arch', 'PREP', 'CONJ', 'UNKN', 'ROMN', 'Coll', 'PRTF', 'PRED', 'INTJ', \n",
    "    'VERB', 'pssv', 'Patr', '2per', 'V-ej', 'intg', 'NUMR', 'COMP', 'Qual', '1per',\n",
    "]\n",
    "\n",
    "@lru_cache(maxsize=2**32)\n",
    "def get_tags(word):\n",
    "    return morph.tag(word)[0].grammemes\n",
    "\n",
    "def get_grammems_sentence(sent):\n",
    "    grammems = Counter(tag for word in sent for tag in get_tags(word))\n",
    "    return {k: v/len(sent) for k, v in grammems.items()}\n",
    "\n",
    "def get_grammems(df): \n",
    "    c2 = df.context_2.str.split().map(get_grammems_sentence).tolist()\n",
    "    c1 = df.context_1.str.split().map(get_grammems_sentence).tolist()\n",
    "    c0 = df.context_0.str.split().map(get_grammems_sentence).tolist()\n",
    "    r = df.reply.str.split().map(get_grammems_sentence).tolist()\n",
    "    \n",
    "    return np.concatenate([\n",
    "        pd.DataFrame(c2, columns=grammes_keys).fillna(0).values,\n",
    "        pd.DataFrame(c1, columns=grammes_keys).fillna(0).values,\n",
    "        pd.DataFrame(c0, columns=grammes_keys).fillna(0).values,\n",
    "        pd.DataFrame(r, columns=grammes_keys).fillna(0).values,\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:39:20.805285Z",
     "start_time": "2018-04-27T09:38:43.049973Z"
    }
   },
   "outputs": [],
   "source": [
    "train_grammems = get_grammems(train)\n",
    "final_grammems = get_grammems(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings diff features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:39:20.813604Z",
     "start_time": "2018-04-27T09:39:20.807691Z"
    }
   },
   "outputs": [],
   "source": [
    "def similarity(v1, v2):\n",
    "    return 1 - cosine(v1, v2)\n",
    "\n",
    "def get_cos_sentence(sent):    \n",
    "    sent = sent.split()\n",
    "    mapped = list(map(fasttext.get_word_vector, sent))\n",
    "    cos = sum(similarity(v1, v2) for v1, v2 in zip(mapped[:-1], mapped[1:]))\n",
    "    return cos / len(sent) if sent else 0\n",
    "\n",
    "def get_cos(df):\n",
    "    return pd.concat([\n",
    "        df.context_2.map(get_cos_sentence),\n",
    "        df.context_1.map(get_cos_sentence),\n",
    "        df.context_0.map(get_cos_sentence),\n",
    "        df.reply.map(get_cos_sentence),\n",
    "    ], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:42:52.208728Z",
     "start_time": "2018-04-27T09:39:20.816119Z"
    }
   },
   "outputs": [],
   "source": [
    "train_cos_diff = get_cos(train)\n",
    "final_cos_diff = get_cos(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence scalar multiplication feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:42:52.229401Z",
     "start_time": "2018-04-27T09:42:52.212775Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sum_sentence(sent):\n",
    "    return sum(map(fasttext.get_word_vector, sent.split()), np.zeros(fasttext_dim))\n",
    "\n",
    "def get_scalar_mul(df):\n",
    "    c2 = np.array(df.context_2.map(get_sum_sentence).tolist())\n",
    "    c1 = np.array(df.context_1.map(get_sum_sentence).tolist())\n",
    "    c0 = np.array(df.context_0.map(get_sum_sentence).tolist())\n",
    "    r = np.array(df.reply.map(get_sum_sentence).tolist())\n",
    "    \n",
    "    # Sure c2 * c1 * c0 * r should be here\n",
    "    # But by mistake I counted c2 twice. \n",
    "    # Yet I only discovered this when I was preparing my code and\n",
    "    # it's too late to change anything now\n",
    "    return c2 * c1 * c2 * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:43:46.734512Z",
     "start_time": "2018-04-27T09:42:52.232636Z"
    }
   },
   "outputs": [],
   "source": [
    "train_scalar_mul = get_scalar_mul(train)\n",
    "final_scalar_mul = get_scalar_mul(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idf counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:43:46.771991Z",
     "start_time": "2018-04-27T09:43:46.746250Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_idf_sentence(sent, vocab, idf_dict):\n",
    "    sum_idf = sum(idf_dict[word] for word in sent if word in vocab)\n",
    "    return sum_idf / len(sent) if sent else 0\n",
    "\n",
    "def get_idf(df, vectorizer):\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    idf_dict = dict(zip(sorted(vectorizer.vocabulary_), vectorizer.idf_))\n",
    "    def idf_weight_getter(sent):\n",
    "        return get_idf_sentence(sent.split(), vocab=vocab, idf_dict=idf_dict)\n",
    "    \n",
    "    return pd.concat([\n",
    "        df.context_2.map(idf_weight_getter),\n",
    "        df.context_2.map(len),\n",
    "        df.context_1.map(idf_weight_getter),\n",
    "        df.context_1.map(len),\n",
    "        df.context_0.map(idf_weight_getter),\n",
    "        df.context_0.map(len),\n",
    "        df.reply.map(idf_weight_getter),\n",
    "        df.reply.map(len),\n",
    "    ], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:43:50.349178Z",
     "start_time": "2018-04-27T09:43:46.789202Z"
    }
   },
   "outputs": [],
   "source": [
    "train_idf_weight = get_idf(train, vectorizer)\n",
    "final_idf_weight = get_idf(final, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:43:50.363213Z",
     "start_time": "2018-04-27T09:43:50.351405Z"
    }
   },
   "outputs": [],
   "source": [
    "def similarity_vetorized(v1, v2):\n",
    "    norm1 = np.sqrt(np.sum(np.square(v1), axis=1))\n",
    "    norm2 = np.sqrt(np.sum(np.square(v2), axis=1))\n",
    "    return np.sum(v1 * v2, axis=1) / norm1 / norm2\n",
    "\n",
    "def get_fasttext_cos_data(df):\n",
    "    c2 = np.array(df.context_2.map(fasttext.get_sentence_vector).tolist())\n",
    "    c1 = np.array(df.context_1.map(fasttext.get_sentence_vector).tolist())\n",
    "    c0 = np.array(df.context_0.map(fasttext.get_sentence_vector).tolist())\n",
    "    r = np.array(df.reply.map(fasttext.get_sentence_vector).tolist())\n",
    "    \n",
    "    return np.nan_to_num(np.vstack([\n",
    "        similarity_vetorized(c2, r),\n",
    "        similarity_vetorized(c1, r),\n",
    "        similarity_vetorized(c0, r),\n",
    "        similarity_vetorized(c2 + c1 + c0, r),\n",
    "    ]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:44:12.466068Z",
     "start_time": "2018-04-27T09:43:50.365964Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fasttext_cos = get_fasttext_cos_data(train)\n",
    "final_fasttext_cos = get_fasttext_cos_data(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:44:12.476203Z",
     "start_time": "2018-04-27T09:44:12.468767Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_fasttext_sum_sent(sent, vocab, idf_dict):\n",
    "    return sum(\n",
    "        (fasttext.get_word_vector(word) * idf_dict[word] for word in sent if word in vocab), \n",
    "        np.zeros(fasttext_dim)\n",
    "    )\n",
    "\n",
    "def get_fasttext_sum(df, vectorizer=None):\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    idf_dict = dict(zip(sorted(vectorizer.vocabulary_), vectorizer.idf_))\n",
    "    def sum_getter(sent):\n",
    "        return get_fasttext_sum_sent(sent.split(), vocab=vocab, idf_dict=idf_dict)\n",
    "    \n",
    "    return (\n",
    "        np.array(df.context_2.map(sum_getter).tolist()),\n",
    "        np.array(df.context_1.map(sum_getter).tolist()),\n",
    "        np.array(df.context_0.map(sum_getter).tolist()),\n",
    "        np.array(df.reply.map(sum_getter).tolist()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:45:14.433392Z",
     "start_time": "2018-04-27T09:44:12.482171Z"
    }
   },
   "outputs": [],
   "source": [
    "train_fasttext_sum_context_2, \\\n",
    "train_fasttext_sum_context_1, \\\n",
    "train_fasttext_sum_context_0, \\\n",
    "train_fasttext_sum_reply = get_fasttext_sum(train, vectorizer)\n",
    "\n",
    "final_fasttext_sum_context_2, \\\n",
    "final_fasttext_sum_context_1, \\\n",
    "final_fasttext_sum_context_0, \\\n",
    "final_fasttext_sum_reply = get_fasttext_sum(final, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:45:14.452787Z",
     "start_time": "2018-04-27T09:45:14.435638Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_intersection(df, vectorizer):\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    idf_dict = dict(zip(sorted(vectorizer.vocabulary_), vectorizer.idf_))\n",
    "\n",
    "    data = []\n",
    "    for tup in df.itertuples():\n",
    "        reply = set(tup.reply.split())\n",
    "        list_c2 = set(tup.context_2.split()) & reply\n",
    "        list_c1 = set(tup.context_1.split()) & reply\n",
    "        list_c0 = set(tup.context_0.split()) & reply\n",
    "        \n",
    "        # Vectors of intersections\n",
    "        inter_c2 = get_fasttext_sum_sent(list_c2, vocab, idf_dict)\n",
    "        inter_c1 = get_fasttext_sum_sent(list_c1, vocab, idf_dict)\n",
    "        inter_c0 = get_fasttext_sum_sent(list_c0, vocab, idf_dict)\n",
    "        \n",
    "        # Idf and number of words in intersections\n",
    "        idf_c2 = get_idf_sentence(list_c2, vocab, idf_dict)\n",
    "        inter_c2 = np.concatenate([inter_c2, [idf_c2, len(list_c2)]])\n",
    "\n",
    "        idf_c1 = get_idf_sentence(list_c1, vocab, idf_dict)\n",
    "        inter_c1 = np.concatenate([inter_c1, [idf_c1, len(list_c1)]])\n",
    "\n",
    "        idf_c0 = get_idf_sentence(list_c0, vocab, idf_dict)\n",
    "        inter_c0 = np.concatenate([inter_c0, [idf_c0, len(list_c0)]])\n",
    "        \n",
    "        data.append((inter_c2, inter_c1, inter_c0))\n",
    "\n",
    "    return map(np.array, zip(*data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:45:29.838044Z",
     "start_time": "2018-04-27T09:45:14.455145Z"
    }
   },
   "outputs": [],
   "source": [
    "train_inter_c2, train_inter_c1, train_inter_c0 = get_intersection(train, vectorizer)\n",
    "final_inter_c2, final_inter_c1, final_inter_c0 = get_intersection(final, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:45:40.276272Z",
     "start_time": "2018-04-27T09:45:29.841847Z"
    }
   },
   "outputs": [],
   "source": [
    "train_r_result = train_fasttext_sum_reply\n",
    "train_c2_result = np.concatenate([train_fasttext_sum_context_2, train_inter_c2], axis=1)\n",
    "train_c1_result = np.concatenate([train_fasttext_sum_context_1, train_inter_c1], axis=1)\n",
    "train_c0_result = np.concatenate([train_fasttext_sum_context_0, train_inter_c0], axis=1)\n",
    "train_features_full = np.concatenate([\n",
    "    train_idf_weight, \n",
    "    train_fasttext_cos,\n",
    "    train_grammems,\n",
    "    train_cos_diff,\n",
    "    train_scalar_mul,\n",
    "], axis=1)\n",
    "\n",
    "final_r_result = final_fasttext_sum_reply\n",
    "final_c2_result = np.concatenate([final_fasttext_sum_context_2, final_inter_c2], axis=1)\n",
    "final_c1_result = np.concatenate([final_fasttext_sum_context_1, final_inter_c1], axis=1)\n",
    "final_c0_result = np.concatenate([final_fasttext_sum_context_0, final_inter_c0], axis=1)\n",
    "final_features_full = np.concatenate([\n",
    "    final_idf_weight, \n",
    "    final_fasttext_cos,\n",
    "    final_grammems,\n",
    "    final_cos_diff,\n",
    "    final_scalar_mul,\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:45:40.325987Z",
     "start_time": "2018-04-27T09:45:40.280584Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "validation_contexts = set(np.random.choice(np.unique(train.context_id), 3500, replace=False))\n",
    "validation_mask = train.context_id.isin(validation_contexts).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:46:08.933859Z",
     "start_time": "2018-04-27T09:45:40.328684Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_train_r_result = train_r_result[~validation_mask]\n",
    "sub_train_c2_result = train_c2_result[~validation_mask]\n",
    "sub_train_c1_result = train_c1_result[~validation_mask]\n",
    "sub_train_c0_result = train_c0_result[~validation_mask]\n",
    "sub_train_features_full = train_features_full[~validation_mask]\n",
    "sub_train_top_vectors = train_top_vectors[~validation_mask]\n",
    "\n",
    "\n",
    "validation_r_result = train_r_result[validation_mask]\n",
    "validation_c2_result = train_c2_result[validation_mask]\n",
    "validation_c1_result = train_c1_result[validation_mask]\n",
    "validation_c0_result = train_c0_result[validation_mask]\n",
    "validation_features_full = train_features_full[validation_mask]\n",
    "validation_top_vectors = train_top_vectors[validation_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:46:09.016061Z",
     "start_time": "2018-04-27T09:46:08.943430Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_train_input = [\n",
    "    sub_train_c2_result, sub_train_c1_result, sub_train_c0_result, \n",
    "    sub_train_r_result, sub_train_features_full, sub_train_top_vectors,\n",
    "]\n",
    "\n",
    "validation_input = [\n",
    "    validation_c2_result, validation_c1_result, validation_c0_result, \n",
    "    validation_r_result, validation_features_full, validation_top_vectors,\n",
    "]\n",
    "\n",
    "final_input = [\n",
    "    final_c2_result, final_c1_result, final_c0_result, \n",
    "    final_r_result, final_features_full, final_top_vectors,\n",
    "]\n",
    "\n",
    "\n",
    "y = to_categorical(train.label)\n",
    "sub_train_y = y[~validation_mask]\n",
    "validation_y = y[validation_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:46:09.047164Z",
     "start_time": "2018-04-27T09:46:09.018812Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nn(dim_c2=402, dim_c1=402, dim_c0=402, dim_r=200, features_dim=616):\n",
    "    input_c2 = Input(shape=(dim_c2,))\n",
    "    input_c1 = Input(shape=(dim_c1,))\n",
    "    input_c0 = Input(shape=(dim_c0,))\n",
    "    input_r = Input(shape=(dim_r,))\n",
    "    input_features = Input(shape=(features_dim,))\n",
    "\n",
    "    dl_c2 = Dense(1024, input_dim=1, activation='relu')(input_c2)\n",
    "    dl_c2 = Dropout(0.5)(dl_c2)\n",
    "    dl_c2 = Dense(512, activation='relu')(dl_c2)\n",
    "    dl_c2 = BatchNormalization()(dl_c2)\n",
    "    \n",
    "    dl_c1 = Dense(1024, input_dim=1, activation='relu')(input_c1)\n",
    "    dl_c1 = Dropout(0.5)(dl_c1)\n",
    "    dl_c1 = Dense(512, activation='relu')(dl_c1)\n",
    "    dl_c1 = BatchNormalization()(dl_c1)\n",
    "    \n",
    "    dl_c0 = Dense(1024, input_dim=1, activation='relu')(input_c0)\n",
    "    dl_c0 = Dropout(0.5)(dl_c0)\n",
    "    dl_c0 = Dense(512, activation='relu')(dl_c0)\n",
    "    dl_c0 = BatchNormalization()(dl_c0)\n",
    "    \n",
    "    dl_r = Dense(1024,input_dim=1, activation='relu')(input_r)\n",
    "    dl_r = Dropout(0.5)(dl_r)\n",
    "    dl_r = Dense(512, activation='relu')(dl_r)\n",
    "    dl_r = BatchNormalization()(dl_r)\n",
    "    \n",
    "    dl_features = Dense(1024,input_dim=1, activation='relu')(input_features)\n",
    "    dl_features = Dropout(0.5)(dl_features)\n",
    "    dl_features = Dense(512, activation='relu')(dl_features)\n",
    "    dl_features = BatchNormalization()(dl_features)\n",
    "    \n",
    "    out1 = Concatenate()([dl_c2, dl_c1, dl_c0, dl_r, dl_features])\n",
    "    \n",
    "    x1 = Dense(512, activation='relu')(out1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "    x1 = Dense(1024, activation='relu')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    \n",
    "    input_top = Input(shape=(20, 200, ))\n",
    "    \n",
    "    cl_t0 = Conv1D(100, 1, activation='relu')(input_top)\n",
    "    cl_t0 = MaxPooling1D(10)(cl_t0)\n",
    "    cl_t0 = Flatten()(cl_t0)\n",
    "    cl_t0 = Dropout(0.5)(cl_t0)\n",
    "    \n",
    "    cl_t1 = Conv1D(100, 2, activation='relu')(input_top)\n",
    "    cl_t1 = MaxPooling1D(7)(cl_t1)\n",
    "    cl_t1 = Flatten()(cl_t1)\n",
    "    cl_t1 = Dropout(0.5)(cl_t1)\n",
    "    \n",
    "    cl_t2 = Conv1D(100, 3, activation='relu')(input_top)\n",
    "    cl_t2 = MaxPooling1D(7)(cl_t2)\n",
    "    cl_t2 = Flatten()(cl_t2)\n",
    "    cl_t2 = Dropout(0.5)(cl_t2)\n",
    "    \n",
    "    cl_t3 = Conv1D(100, 4, activation='relu')(input_top)\n",
    "    cl_t3 = MaxPooling1D(7)(cl_t3)\n",
    "    cl_t3 = Flatten()(cl_t3)\n",
    "    cl_t3 = Dropout(0.5)(cl_t3)\n",
    "    \n",
    "    cl_t4 = Conv1D(100, 5, activation='relu')(input_top)\n",
    "    cl_t4 = MaxPooling1D(7)(cl_t4)\n",
    "    cl_t4 = Flatten()(cl_t4)\n",
    "    cl_t4 = Dropout(0.5)(cl_t4)\n",
    "    \n",
    "    out2 = Concatenate()([cl_t0, cl_t1, cl_t2, cl_t3, cl_t4])\n",
    "\n",
    "    x2 = Dense(512, activation='relu')(out2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "    x2 = Dense(1024, activation='relu')(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    \n",
    "    out = Concatenate()([x1, x2])\n",
    "  \n",
    "    x = Dense(1024, activation='relu')(out)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[input_c2, input_c1, input_c0, input_r, input_features, input_top], \n",
    "        outputs=[x],\n",
    "    )\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:54:27.008112Z",
     "start_time": "2018-04-27T09:54:15.388257Z"
    }
   },
   "outputs": [],
   "source": [
    "if REDO_NN:\n",
    "    nn = get_nn()\n",
    "    nn.fit(\n",
    "        sub_train_input, sub_train_y, sample_weight=train.confidence.values[~validation_mask],\n",
    "        validation_data=(validation_input, validation_y),\n",
    "        batch_size=64,\n",
    "        nb_epoch=15,\n",
    "    )\n",
    "else:\n",
    "    nn = load_model('data/final_model_weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T09:58:02.188698Z",
     "start_time": "2018-04-27T09:56:33.266801Z"
    }
   },
   "outputs": [],
   "source": [
    "final['nn_score'] = nn.predict(final_input, batch_size=512, verbose=1).dot([-1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T10:01:23.137540Z",
     "start_time": "2018-04-27T10:01:23.129622Z"
    }
   },
   "outputs": [],
   "source": [
    "final['nn_score'] = np.load('ILYA_final_88.npy').dot([-1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T10:01:25.933106Z",
     "start_time": "2018-04-27T10:01:25.322481Z"
    }
   },
   "outputs": [],
   "source": [
    "final[['context_id', 'reply_id', 'nn_score']].to_csv('nn_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T10:01:37.528651Z",
     "start_time": "2018-04-27T10:01:26.474393Z"
    }
   },
   "outputs": [],
   "source": [
    "sub = final.groupby('context_id').apply(\n",
    "    lambda x: x.sort_values('nn_score', ascending=False).reply_id\n",
    ").reset_index(level=0)\n",
    "\n",
    "sub.to_csv('nn-final-sub.tsv', index=False, header=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
